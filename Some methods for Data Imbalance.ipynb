{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 14. 데이터 불균형\n\n데이터 불균형이란 데이터 세트 내의 클래스의 분포가 불균형한 것을 의미한다. 데이터 불균형은 특정 클래스에 과적합 되는 현상을 유발할 수 있기 때문에 반드시 해결해야 하는 문제다.","metadata":{"id":"EyfIVxAbURRU"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"oUePdtdOYP98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cd/content/drive/MyDrive/deeplearningbro/pytorch","metadata":{"id":"ca4QSHzOYQaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 14.1 Weighted Random Sampling\nSGD base를 쓰기 때문에 일정한 배치가 들어옴 -> 전체 데이터는 불균형하더라도 배치를 균형데이터로 뽑는 것 (각 클래스들의 데이터들이 같은 확률로 뽑히게끔)","metadata":{"id":"fMTc4laPU1BB"}},{"cell_type":"markdown","source":"가중 샘플링 방식은 확률적 샘플링이기 때문에 전체 데이터 사용이 보장이 안 됩니다. 그럼에도 균형된 배치를 사용할 수 있기 때문에 랜덤 샘플링 보다는 더 좋은 성능이 이끌어 냅니다. 아주 오래 전에 저도 실제 연구에서 고민했던 질문이라 반갑습니다. 제가 시도 했던 방법은 크게 다음과 같습니다.\n\n1. 가중 샘플링을 사용하되 데이터를 더 많이 활용할 수 있도록 에폭 수를 늘려 샘플링을 보다 많이 하게 한다.\n\n2. 클래스마다 데이터 증식의 개수를 다르게 하여 불균형을 줄여준 뒤 무작위 샘플링을 진행한다. (oversampling)\n\n3. 가장 수가 작은 클래스 기준으로 다른 클래스를 부분집합으로 나누어 매 에폭마다 돌아가면서 사용한다. (변형 된 undersampling)\n\n4. 클래스가 가장 큰 데이터에 대해서 데이터 분석을 하여 일부를 추출하여 학습에 사용한다. 즉, 오히려 큰 클래스에 대해서 데이터가 덜 사용 됨을 인정하고 중요하다고 생각하는 데이터만 주입한다. (데이터 분석 후 가중 샘플링)\n\n저의 경우 4번이 가장 큰 효과가 있었습니다","metadata":{}},{"cell_type":"code","source":"# 각 클래스의 비율을 정하여 뽑힐 확률에 대한 가중치를 산정한다.\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport numpy as np\n\ndef make_weights_for_balanced_classes(img, nclasses):\n\n    labels = []\n    for i in range(len(img)):\n        labels.append(img[i][1])\n\n    label_array = np.array(labels)\n    total = len(labels)\n\n    count_list = []\n    for cls in range(nclasses):\n        count = len(np.where(label_array == cls)[0])\n        count_list.append(total/count)\n\n    weights = []\n    for label in label_array:\n        weights.append(count_list[label])\n\n    return weights\n","metadata":{"id":"jrBtSTGsU66B","execution":{"iopub.status.busy":"2022-04-12T04:36:31.098091Z","iopub.execute_input":"2022-04-12T04:36:31.098399Z","iopub.status.idle":"2022-04-12T04:36:31.104384Z","shell.execute_reply.started":"2022-04-12T04:36:31.098371Z","shell.execute_reply":"2022-04-12T04:36:31.103722Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pwd\n../input/dl-class/class","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:36:41.083819Z","iopub.execute_input":"2022-04-12T04:36:41.084384Z","iopub.status.idle":"2022-04-12T04:36:41.748282Z","shell.execute_reply.started":"2022-04-12T04:36:41.084344Z","shell.execute_reply":"2022-04-12T04:36:41.747479Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"trainset.imgs\ntrainset.classes","metadata":{"execution":{"iopub.status.busy":"2022-04-12T04:38:52.722606Z","iopub.execute_input":"2022-04-12T04:38:52.723061Z","iopub.status.idle":"2022-04-12T04:38:52.728284Z","shell.execute_reply.started":"2022-04-12T04:38:52.723025Z","shell.execute_reply":"2022-04-12T04:38:52.727592Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainset = torchvision.datasets.ImageFolder(root='../input/dl-class/class', transform=transf) # Custom 데이터 세트 불러오기\n\nweights = make_weights_for_balanced_classes(trainset.imgs, len(trainset.classes)) # 가중치 계산\nweights = torch.DoubleTensor(weights) # 구한 weight를 텐서로 변환\nsampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights)) # 샘플링 방법 정의\n\ntrainloader = DataLoader(trainset, batch_size=16, sampler=sampler) # 데이터 로더 정의","metadata":{"id":"DwVyuopAVSiU","execution":{"iopub.status.busy":"2022-04-12T04:37:32.622808Z","iopub.execute_input":"2022-04-12T04:37:32.623086Z","iopub.status.idle":"2022-04-12T04:37:32.647470Z","shell.execute_reply.started":"2022-04-12T04:37:32.623057Z","shell.execute_reply":"2022-04-12T04:37:32.646816Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## 14.2 Weighted Loss Function\n\n각각의 class마다 loss가 계산될 때 가중치를 곱해주는 방법\n- ex : 10개의 클래스가 있다고 했을 때, loss 계산할 때 가장 큰 값에는 작은 값을 곱해주고 가장 작은 값에는 큰 값을 곱해줌으로써 데이터의 개수마다 loss 크기를 다르게 설정해줌","metadata":{"id":"7lnvdVLDU9XD","execution":{"iopub.status.busy":"2022-04-12T04:31:48.259122Z","iopub.execute_input":"2022-04-12T04:31:48.259656Z","iopub.status.idle":"2022-04-12T04:31:48.263291Z","shell.execute_reply.started":"2022-04-12T04:31:48.259618Z","shell.execute_reply":"2022-04-12T04:31:48.262039Z"}}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nnum_ins = [40,45,30,62,70,153,395,46,75,194]\nweights = [1-(x/sum(num_ins)) for x in num_ins]\nclass_weights = torch.FloatTensor(weights).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"id":"RT54D87fVAk5","execution":{"iopub.status.busy":"2022-04-12T04:41:51.600861Z","iopub.execute_input":"2022-04-12T04:41:51.601269Z","iopub.status.idle":"2022-04-12T04:41:54.674558Z","shell.execute_reply.started":"2022-04-12T04:41:51.601227Z","shell.execute_reply":"2022-04-12T04:41:54.673819Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## 14.3 Data Augmentation\n\nOverfitting 방지를 위해서 쓰이지만 Data Imbalance 이슈에서도 쓰일 수 있음.\n\n이미지를 더 늘리는 방법","metadata":{"id":"ztEPe1J_VBD4","execution":{"iopub.status.busy":"2022-04-12T04:43:25.626694Z","iopub.execute_input":"2022-04-12T04:43:25.627375Z","iopub.status.idle":"2022-04-12T04:43:25.632224Z","shell.execute_reply.started":"2022-04-12T04:43:25.627339Z","shell.execute_reply":"2022-04-12T04:43:25.631207Z"}}},{"cell_type":"code","source":"import torchvision.transforms as tr\nimport PIL\n\ntransf = tr.Compose([tr.ToPILImage(), tr.RandomCrop(60), tr.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                 tr.RandomHorizontalFlip(),\n                 tr.RandomRotation(10, resample=PIL.Image.BILINEAR),\n                 tr.ToTensor()\n                 ])","metadata":{"id":"fYiPkl_nVGDs","execution":{"iopub.status.busy":"2022-04-12T04:45:18.926257Z","iopub.execute_input":"2022-04-12T04:45:18.926506Z","iopub.status.idle":"2022-04-12T04:45:18.933153Z","shell.execute_reply.started":"2022-04-12T04:45:18.926476Z","shell.execute_reply":"2022-04-12T04:45:18.932088Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 14.4 Confusion Matrix\n\n직접적인 해결 방법은 아닌데, 학습을 마치고 나온 실제 값과 예측 값을 가지고 Matrix을 그려보는 것\n- matrix를 그려봤을 때 class 중에서 어떤 클래스가 많이 맞췄고, 적게 맞췄는지를 파악해서 특정 클래스에 가중치를 더 줄 수 있고, 특정 클래스에 aug를 더 할 수도 있고, 이런 식으로 결과를 보고 다음 액션을 취할 수 있게 해준다.","metadata":{"id":"H_GxDZBXVG37"}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nactual = [1,1,1,1,1,1,0,0,0,0,0,2,2,2,2,2]\nprediction = [1,1,1,0,1,1,0,0,0,1,0,2,2,2,1,1]\nc_mat = confusion_matrix(actual, prediction) # 실제 라벨, 예측값\nplt.figure(figsize = (8,6))\nsns.heatmap(c_mat, annot=True, fmt=\"d\", cmap='Blues',linewidths=.5)\nb, t = plt.ylim() \nb += 0.5 \nt -= 0.5 \nplt.ylim(b, t) \nplt.savefig('confusion_matrix.png')\nplt.show()\n\n","metadata":{"id":"MVL_3giLWpuK","executionInfo":{"status":"ok","timestamp":1608741088260,"user_tz":-60,"elapsed":2807,"user":{"displayName":"딥러닝호형","photoUrl":"","userId":"11263585794403583722"}},"outputId":"6cfa043f-ea7e-4e0c-9d05-64030bdbbd61","execution":{"iopub.status.busy":"2022-04-12T04:45:46.820281Z","iopub.execute_input":"2022-04-12T04:45:46.820541Z","iopub.status.idle":"2022-04-12T04:45:48.088300Z","shell.execute_reply.started":"2022-04-12T04:45:46.820512Z","shell.execute_reply":"2022-04-12T04:45:48.087637Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"_WgWgfF-XGys"},"execution_count":null,"outputs":[]}]}